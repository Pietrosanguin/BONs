{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EMBEDDINGS CREATION\n",
    "\n",
    "This is the script where we make use of the results of the script **Preprocessing.ipynb**. Finally we can create our embeddings starting from our data. The embeddings will also recive an additional formatting preprocessing in order to make them suitable to be used with the amplpy library, which will be the next step of our work. Our embeddings will, indeed, be feed to two different optimization algorithms as reported in the thesis work. The module used is Gensim word2vec.\n",
    "\n",
    "Now since all this work has the aim to imporve an alignment technique we will refer to source embedding space and target embedding space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following names are the names of the files that we preprocessed and that now become the source and the target embedding spaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO MODIFY IF YOU WANT TO CHANGE THE EMBEDDINGS\n",
    "SOURCE_PREPROCESSED_TXT = 'p2_0side_2959files_preprocessed.txt'\n",
    "TARGET_PREPROCESSED_TXT = 'p2_2side_2959files_preprocessed.txt'\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Name of the json file where we will save the word2id information\n",
    "NAME_JSON_FILE = f'{SOURCE_PREPROCESSED_TXT.replace(\"preprocessed.txt\", \"\")}' +'&_' + f'{TARGET_PREPROCESSED_TXT.replace(\"_preprocessed.txt\", \"\")}' + \"_word2id.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_preprocessed_txt(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        text = file.read()\n",
    "    return text\n",
    "\n",
    "#Load the source file\n",
    "source_text = load_preprocessed_txt(SOURCE_PREPROCESSED_TXT)\n",
    "\n",
    "#Load the target file\n",
    "target_text = load_preprocessed_txt(TARGET_PREPROCESSED_TXT)\n",
    "\n",
    "\n",
    "# Suddividi il testo in token\n",
    "tokenized_source_text = source_text.split()\n",
    "tokenized_terget_text = target_text.split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we get into the real model definition, the two embeddings should be created applying the same methodology.\n",
    "The following parameters are defined. For further reference see the thesis or Gensim library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "VECTOR_SIZE = 300\n",
    "WINDOW_SIZE = 5\n",
    "EPOCHS = 5\n",
    "#if we want the Skip-gram model (1) or the CBOW model (0)\n",
    "SG = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_model = Word2Vec([tokenized_source_text], vector_size=VECTOR_SIZE, window = WINDOW_SIZE, seed=42, epochs=EPOCHS, sg=SG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_model = Word2Vec([tokenized_terget_text], vector_size=VECTOR_SIZE, window = WINDOW_SIZE, seed=42, epochs=EPOCHS, sg=SG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Saving the models\n",
    "\n",
    "Gensim has its own format in order to save and load embedding models.\n",
    "In the following we report the script to save the existing models and the way to load them.\n",
    "The model is saved inside a directory called **tmp** which is created if it does not exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tempfile\n",
    "\n",
    "# Specify the folder where you want to create the temporary file\n",
    "tmp_dir = \"tmp\"\n",
    "\n",
    "# Ensure the temporary directory exists, create if not\n",
    "os.makedirs(tmp_dir, exist_ok=True)\n",
    "\n",
    "def save_genism_model(model, name):\n",
    "    # Create a temporary file with a specific prefix in the specified directory\n",
    "    with tempfile.NamedTemporaryFile(prefix=f'gensim-model-{name.replace(\"_preprocessed.txt\", \"\")}', dir=tmp_dir, delete=False) as tmp:\n",
    "        temporary_filepath = tmp.name\n",
    "        model.save(temporary_filepath)\n",
    "\n",
    "        #\n",
    "        # The model is now safely stored in the filepath.\n",
    "        # You can copy it to other machines, share it with others, etc.\n",
    "\n",
    "\n",
    "save_genism_model(source_model, SOURCE_PREPROCESSED_TXT)\n",
    "save_genism_model(target_model, TARGET_PREPROCESSED_TXT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we check how many words the two models have in common"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4367"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "common_words = set(source_model.wv.index_to_key) & set(target_model.wv.index_to_key)\n",
    "len(common_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.2 Creation of the matrices and word2id dictionary\n",
    "\n",
    "For our next use we now have to create the numpy matrices and the word2id dictionary. In the numpy matrices, indeed, we will lost the information about which word is the vector representig. The creation of the dictionary prevent this to happend since every line of the matrix correspond to a word. The reference line word is saved inside the word2id dictionary (that will be a json file)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The word corresponding to row 11 is: reti\n"
     ]
    }
   ],
   "source": [
    "# Creation of a list with the common words\n",
    "common_words_list = [word for word in common_words]\n",
    "\n",
    "# Now you have a list of words that will correspond to the rows in the X and Y matrices\n",
    "\n",
    "#Example\n",
    "# Suppose col_idx is the index of the line you are interested in\n",
    "col_idx = 11  \n",
    "\n",
    "# Now you can get the corresponding word from the list\n",
    "parola_corrispondente = common_words_list[col_idx]\n",
    "print(f\"The word corresponding to row {col_idx} is: {parola_corrispondente}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2.1 Creation of the json file\n",
    "\n",
    "The json file is the word2id dictionary and it is fundamental for the netxt step that will be the alignment via amplpy modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Creare un dizionario con le parole comuni come chiavi e gli indici come valori\n",
    "word_index_dict = {word: index for index, word in enumerate(common_words_list)}\n",
    "\n",
    "# Salvare il dizionario in un file JSON\n",
    "with open(NAME_JSON_FILE, 'w') as file:\n",
    "    json.dump(word_index_dict, file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2.2 Creation of the embedding matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([source_model.wv[word] for word in common_words])\n",
    "Y = np.array([target_model.wv[word] for word in common_words])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving the matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(f'X_source_embeddings_{SOURCE_PREPROCESSED_TXT.replace(\"_preprocessed.txt\", \"\")}', X)\n",
    "\n",
    "np.save(f'Y_target_embeddings_{TARGET_PREPROCESSED_TXT.replace(\"_preprocessed.txt\", \"\")}', Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These matrices and the json file will be necessary for the next step involving the use of amplpy. See the script **ampl_test_norm1.ipynb**."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
